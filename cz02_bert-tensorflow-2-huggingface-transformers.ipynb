{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "bert-tensorflow-2-huggingface-transformers.ipynb",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Toxic Comment Classification Challenge\n",
        "## BERT - TensorFlow 2 & Hugging Face Transformers Library"
      ],
      "metadata": {
        "id": "layZggiLcLw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==2.3.0"
      ],
      "metadata": {
        "id": "SuY02mplBrDy",
        "outputId": "aa157869-0ece-4007-bed1-9f962432f740",
        "execution": {
          "iopub.status.busy": "2022-07-13T22:20:27.846379Z",
          "iopub.execute_input": "2022-07-13T22:20:27.846631Z",
          "iopub.status.idle": "2022-07-13T22:20:38.269378Z",
          "shell.execute_reply.started": "2022-07-13T22:20:27.846582Z",
          "shell.execute_reply": "2022-07-13T22:20:38.268355Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==2.3.0\n",
            "  Downloading transformers-2.3.0-py3-none-any.whl (447 kB)\n",
            "\u001b[K     |████████████████████████████████| 447 kB 25.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (4.64.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 47.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (1.21.6)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.24.36-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 39.8 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 52.6 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.28.0,>=1.27.36\n",
            "  Downloading botocore-1.27.36-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 48.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.36->boto3->transformers==2.3.0) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 70.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.36->boto3->transformers==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.3.0) (2022.6.15)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 63.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.3.0) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.3.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.3.0) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=f2efdb702213d1eec2bb9d2ce4dc22f4877dbdd9e5db38c514dcb03f0901388c\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 2891, in parseImpl\n",
            "    if instring[loc] == self.firstMatchChar and instring.startswith(self.match, loc):\n",
            "IndexError: string index out of range\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 385, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 515, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 103, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 45, in create_package_set_from_installed\n",
            "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3033, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in parse_requirements\n",
            "    yield Requirement(line)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3101, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/requirements.py\", line 113, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1943, in parseString\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4254, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4849, in parseImpl\n",
            "    loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4462, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4781, in parseImpl\n",
            "    return super(ZeroOrMore, self).parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4697, in parseImpl\n",
            "    loc, tokens = self_expr_parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4052, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4254, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1685, in _parseNoCache\n",
            "    raise ParseException(instring, len(instring), self.errmsg, self)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 304, in __init__\n",
            "    def __init__(self, pstr, loc=0, msg=None, elem=None):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 71, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 104, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 183, in _main\n",
            "    except PreviousBuildDirError as exc:\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_directory = '../input/jigsaw-toxic-comment-classification-challenge'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-13T22:20:38.275397Z",
          "iopub.execute_input": "2022-07-13T22:20:38.277853Z",
          "iopub.status.idle": "2022-07-13T22:20:38.284181Z",
          "shell.execute_reply.started": "2022-07-13T22:20:38.277802Z",
          "shell.execute_reply": "2022-07-13T22:20:38.283154Z"
        },
        "trusted": true,
        "id": "Kl2dveticLw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!unzip {dataset_directory}/train.csv.zip -d data/\n",
        "!unzip {dataset_directory}/test.csv.zip  -d data/\n",
        "!unzip {dataset_directory}/test_labels.csv.zip  -d data/\n",
        "!unzip {dataset_directory}/sample_submission.csv.zip  -d data/"
      ],
      "metadata": {
        "id": "5lawOFYO_KUV",
        "execution": {
          "iopub.status.busy": "2022-07-13T22:20:38.289252Z",
          "iopub.execute_input": "2022-07-13T22:20:38.291613Z",
          "iopub.status.idle": "2022-07-13T22:20:43.599679Z",
          "shell.execute_reply.started": "2022-07-13T22:20:38.291565Z",
          "shell.execute_reply": "2022-07-13T22:20:43.598970Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "aGa2jmNmCjXf",
        "execution": {
          "iopub.status.busy": "2022-07-13T22:20:43.603004Z",
          "iopub.execute_input": "2022-07-13T22:20:43.603249Z",
          "iopub.status.idle": "2022-07-13T22:20:49.036195Z",
          "shell.execute_reply.started": "2022-07-13T22:20:43.603205Z",
          "shell.execute_reply": "2022-07-13T22:20:49.035459Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Pipeline\n",
        "- Loading the datasets from CSVs\n",
        "- Preprocessing (Tokenization, Truncation & Padding)\n",
        "- Creating efficient data pipelines using tf.data"
      ],
      "metadata": {
        "id": "VGuLd_nRcLw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = 'data/train.csv'\n",
        "test_path = 'data/test.csv'\n",
        "test_labels_path = 'data/test_labels.csv'\n",
        "subm_path = 'data/sample_submission.csv'"
      ],
      "metadata": {
        "id": "5FuPBMEX_Lxv",
        "execution": {
          "iopub.status.busy": "2022-07-13T22:20:49.040153Z",
          "iopub.execute_input": "2022-07-13T22:20:49.040392Z",
          "iopub.status.idle": "2022-07-13T22:20:49.045481Z",
          "shell.execute_reply.started": "2022-07-13T22:20:49.040346Z",
          "shell.execute_reply": "2022-07-13T22:20:49.043213Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_test = pd.read_csv(test_path)\n",
        "df_test_labels = pd.read_csv(test_labels_path)\n",
        "df_test_labels = df_test_labels.set_index('id')\n",
        "\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "50uZZZzH_NDV",
        "execution": {
          "iopub.status.busy": "2022-07-13T22:20:49.047109Z",
          "iopub.execute_input": "2022-07-13T22:20:49.047554Z",
          "iopub.status.idle": "2022-07-13T22:20:50.783599Z",
          "shell.execute_reply.started": "2022-07-13T22:20:49.047361Z",
          "shell.execute_reply": "2022-07-13T22:20:50.782877Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\n",
        "MAX_LEN = 128\n",
        "\n",
        "def tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n",
        "    tokenized_sentences = []\n",
        "\n",
        "    for sentence in tqdm(sentences):\n",
        "        tokenized_sentence = tokenizer.encode(\n",
        "                            sentence,                  # Sentence to encode.\n",
        "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                            max_length = max_seq_len,  # Truncate all sentences.\n",
        "                    )\n",
        "        \n",
        "        tokenized_sentences.append(tokenized_sentence)\n",
        "\n",
        "    return tokenized_sentences\n",
        "\n",
        "def create_attention_masks(tokenized_and_padded_sentences):\n",
        "    attention_masks = []\n",
        "\n",
        "    for sentence in tokenized_and_padded_sentences:\n",
        "        att_mask = [int(token_id > 0) for token_id in sentence]\n",
        "        attention_masks.append(att_mask)\n",
        "\n",
        "    return np.asarray(attention_masks)\n",
        "\n",
        "input_ids = tokenize_sentences(df_train['comment_text'], tokenizer, MAX_LEN)\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "attention_masks = create_attention_masks(input_ids)"
      ],
      "metadata": {
        "id": "g-8WnPfD_35q",
        "execution": {
          "iopub.status.busy": "2022-07-13T22:20:50.785006Z",
          "iopub.execute_input": "2022-07-13T22:20:50.785922Z",
          "iopub.status.idle": "2022-07-13T22:28:14.113084Z",
          "shell.execute_reply.started": "2022-07-13T22:20:50.785258Z",
          "shell.execute_reply": "2022-07-13T22:28:14.112323Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "labels =  df_train[label_cols].values\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=0, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=0, test_size=0.1)\n",
        "\n",
        "train_size = len(train_inputs)\n",
        "validation_size = len(validation_inputs)"
      ],
      "metadata": {
        "id": "bMumeio9FFds",
        "execution": {
          "iopub.status.busy": "2022-07-13T22:28:14.114415Z",
          "iopub.execute_input": "2022-07-13T22:28:14.114710Z",
          "iopub.status.idle": "2022-07-13T22:28:14.299137Z",
          "shell.execute_reply.started": "2022-07-13T22:28:14.114652Z",
          "shell.execute_reply": "2022-07-13T22:28:14.298371Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "NR_EPOCHS = 1\n",
        "\n",
        "def create_dataset(data_tuple, epochs=1, batch_size=32, buffer_size=10000, train=True):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n",
        "    if train:\n",
        "        dataset = dataset.shuffle(buffer_size=buffer_size)\n",
        "    dataset = dataset.repeat(epochs)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    if train:\n",
        "        dataset = dataset.prefetch(1)\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "train_dataset = create_dataset((train_inputs, train_masks, train_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\n",
        "validation_dataset = create_dataset((validation_inputs, validation_masks, validation_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "H99t9YNDFcRq",
        "execution": {
          "iopub.status.busy": "2022-07-13T22:28:14.300573Z",
          "iopub.execute_input": "2022-07-13T22:28:14.301140Z",
          "iopub.status.idle": "2022-07-13T22:28:21.388035Z",
          "shell.execute_reply.started": "2022-07-13T22:28:14.300876Z",
          "shell.execute_reply": "2022-07-13T22:28:21.387158Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. BERT Model\n",
        "- Load the pretrained BERT base-model from Transformers library\n",
        "- Take the first hidden-state from BERT output (corresponding to CLS token) and feed it into a Dense layer with 6 neurons and sigmoid activation (Classifier). The outputs of this layer can be interpreted as probabilities for each of the 6 classes."
      ],
      "metadata": {
        "id": "gBs2mFOocLw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertModel\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "class BertClassifier(tf.keras.Model):    \n",
        "    def __init__(self, bert: TFBertModel, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.classifier = Dense(num_classes, activation='sigmoid')\n",
        "        \n",
        "    @tf.function\n",
        "    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids,\n",
        "                               attention_mask=attention_mask,\n",
        "                               token_type_ids=token_type_ids,\n",
        "                               position_ids=position_ids,\n",
        "                               head_mask=head_mask)\n",
        "        cls_output = outputs[1]\n",
        "        cls_output = self.classifier(cls_output)\n",
        "                \n",
        "        return cls_output\n",
        "\n",
        "model = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(label_cols))"
      ],
      "metadata": {
        "id": "3Zp6X7meF_T5",
        "execution": {
          "iopub.status.busy": "2022-07-13T22:28:21.389376Z",
          "iopub.execute_input": "2022-07-13T22:28:21.389653Z",
          "iopub.status.idle": "2022-07-13T22:28:39.815649Z",
          "shell.execute_reply.started": "2022-07-13T22:28:21.389610Z",
          "shell.execute_reply": "2022-07-13T22:28:39.814615Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training Loop\n",
        "- Use BinaryCrossentropy as loss function (is calculated for each of the output 6 output neurons ...that's like training 6 binary classification tasks at the same time) \n",
        "- Use the AdamW optimizer with 1-cycle-policy from the Transformers library\n",
        "- AUC evaluation metrics"
      ],
      "metadata": {
        "id": "vKYl3uH5cLw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from transformers import create_optimizer\n",
        "\n",
        "steps_per_epoch = train_size // BATCH_SIZE\n",
        "validation_steps = validation_size // BATCH_SIZE\n",
        "\n",
        "# | Loss Function\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "validation_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "\n",
        "# | Optimizer (with 1-cycle-policy)\n",
        "warmup_steps = steps_per_epoch // 3\n",
        "total_steps = steps_per_epoch * NR_EPOCHS - warmup_steps\n",
        "optimizer = create_optimizer(init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)\n",
        "\n",
        "# | Metrics\n",
        "train_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n",
        "validation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, token_ids, masks, labels):\n",
        "    labels = tf.dtypes.cast(labels, tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(token_ids, attention_mask=masks)\n",
        "        loss = loss_object(labels, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables), 1.0)\n",
        "\n",
        "    train_loss(loss)\n",
        "\n",
        "    for i, auc in enumerate(train_auc_metrics):\n",
        "        auc.update_state(labels[:,i], predictions[:,i])\n",
        "        \n",
        "@tf.function\n",
        "def validation_step(model, token_ids, masks, labels):\n",
        "    labels = tf.dtypes.cast(labels, tf.float32)\n",
        "\n",
        "    predictions = model(token_ids, attention_mask=masks, training=False)\n",
        "    v_loss = loss_object(labels, predictions)\n",
        "\n",
        "    validation_loss(v_loss)\n",
        "    for i, auc in enumerate(validation_auc_metrics):\n",
        "        auc.update_state(labels[:,i], predictions[:,i])\n",
        "                                              \n",
        "def train(model, train_dataset, val_dataset, train_steps_per_epoch, val_steps_per_epoch, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        print('=' * 50, f\"EPOCH {epoch}\", '=' * 50)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        for i, (token_ids, masks, labels) in enumerate(tqdm(train_dataset, total=train_steps_per_epoch)):\n",
        "            train_step(model, token_ids, masks, labels)\n",
        "            if i % 1000 == 0:\n",
        "                print(f'\\nTrain Step: {i}, Loss: {train_loss.result()}')\n",
        "                for i, label_name in enumerate(label_cols):\n",
        "                    print(f\"{label_name} roc_auc {train_auc_metrics[i].result()}\")\n",
        "                    train_auc_metrics[i].reset_states()\n",
        "        \n",
        "        for i, (token_ids, masks, labels) in enumerate(tqdm(val_dataset, total=val_steps_per_epoch)):\n",
        "            validation_step(model, token_ids, masks, labels)\n",
        "\n",
        "        print(f'\\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\n",
        "\n",
        "        for i, label_name in enumerate(label_cols):\n",
        "            print(f\"{label_name} roc_auc {validation_auc_metrics[i].result()}\")\n",
        "            validation_auc_metrics[i].reset_states()\n",
        "\n",
        "        print('\\n')\n",
        "\n",
        "        \n",
        "train(model, train_dataset, validation_dataset, train_steps_per_epoch=steps_per_epoch, val_steps_per_epoch=validation_steps, epochs=NR_EPOCHS)"
      ],
      "metadata": {
        "id": "cJsVaX4qqoM0",
        "execution": {
          "iopub.status.busy": "2022-07-13T22:28:39.817564Z",
          "iopub.execute_input": "2022-07-13T22:28:39.817911Z",
          "iopub.status.idle": "2022-07-13T23:02:43.776885Z",
          "shell.execute_reply.started": "2022-07-13T22:28:39.817849Z",
          "shell.execute_reply": "2022-07-13T23:02:43.775894Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Run predictions on test-set & save submission"
      ],
      "metadata": {
        "id": "5-eEOd0fcLxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_ids = tokenize_sentences(df_test['comment_text'], tokenizer, MAX_LEN)\n",
        "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "test_attention_masks = create_attention_masks(test_input_ids)"
      ],
      "metadata": {
        "id": "cdShHpPfH3es",
        "execution": {
          "iopub.status.busy": "2022-07-13T23:02:43.778226Z",
          "iopub.execute_input": "2022-07-13T23:02:43.778501Z",
          "iopub.status.idle": "2022-07-13T23:09:22.724219Z",
          "shell.execute_reply.started": "2022-07-13T23:02:43.778455Z",
          "shell.execute_reply": "2022-07-13T23:09:22.722962Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_BATCH_SIZE = 32\n",
        "test_steps = len(df_test) // TEST_BATCH_SIZE\n",
        "\n",
        "test_dataset = create_dataset((test_input_ids, test_attention_masks), batch_size=TEST_BATCH_SIZE, train=False, epochs=1)\n",
        "\n",
        "df_submission = pd.read_csv(subm_path, index_col='id')\n",
        "\n",
        "for i, (token_ids, masks) in enumerate(tqdm(test_dataset, total=test_steps)):\n",
        "    sample_ids = df_test.iloc[i*TEST_BATCH_SIZE:(i+1)*TEST_BATCH_SIZE]['id']\n",
        "    predictions = model(token_ids, attention_mask=masks).numpy()\n",
        "\n",
        "    df_submission.loc[sample_ids, label_cols] = predictions"
      ],
      "metadata": {
        "id": "RUAKdp2EKYlZ",
        "execution": {
          "iopub.status.busy": "2022-07-13T23:09:22.725681Z",
          "iopub.execute_input": "2022-07-13T23:09:22.725978Z",
          "iopub.status.idle": "2022-07-13T23:21:25.708845Z",
          "shell.execute_reply.started": "2022-07-13T23:09:22.725932Z",
          "shell.execute_reply": "2022-07-13T23:21:25.708086Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_submission.to_csv('submission.csv')"
      ],
      "metadata": {
        "id": "JgAv8do3PYYX",
        "execution": {
          "iopub.status.busy": "2022-07-13T23:22:56.081828Z",
          "iopub.execute_input": "2022-07-13T23:22:56.082296Z",
          "iopub.status.idle": "2022-07-13T23:22:58.571671Z",
          "shell.execute_reply.started": "2022-07-13T23:22:56.082115Z",
          "shell.execute_reply": "2022-07-13T23:22:58.570915Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_submission"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-13T23:26:48.606342Z",
          "iopub.execute_input": "2022-07-13T23:26:48.606667Z",
          "iopub.status.idle": "2022-07-13T23:26:48.631676Z",
          "shell.execute_reply.started": "2022-07-13T23:26:48.606602Z",
          "shell.execute_reply": "2022-07-13T23:26:48.630413Z"
        },
        "trusted": true,
        "id": "jGNLDN2NcLxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KqQmqO7LcLxC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}