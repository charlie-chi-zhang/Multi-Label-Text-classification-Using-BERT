{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Toxic Comment Classification Challenge\n## BERT - TensorFlow 2 & Hugging Face Transformers Library","metadata":{}},{"cell_type":"code","source":"!pip install transformers==2.3.0","metadata":{"id":"SuY02mplBrDy","outputId":"7812630a-8a77-4cdf-b99b-892778ca3898","execution":{"iopub.status.busy":"2022-07-13T22:20:27.846379Z","iopub.execute_input":"2022-07-13T22:20:27.846631Z","iopub.status.idle":"2022-07-13T22:20:38.269378Z","shell.execute_reply.started":"2022-07-13T22:20:27.846582Z","shell.execute_reply":"2022-07-13T22:20:38.268355Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"dataset_directory = '../input/jigsaw-toxic-comment-classification-challenge'","metadata":{"execution":{"iopub.status.busy":"2022-07-13T22:20:38.275397Z","iopub.execute_input":"2022-07-13T22:20:38.277853Z","iopub.status.idle":"2022-07-13T22:20:38.284181Z","shell.execute_reply.started":"2022-07-13T22:20:38.277802Z","shell.execute_reply":"2022-07-13T22:20:38.283154Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!mkdir data\n!unzip {dataset_directory}/train.csv.zip -d data/\n!unzip {dataset_directory}/test.csv.zip  -d data/\n!unzip {dataset_directory}/test_labels.csv.zip  -d data/\n!unzip {dataset_directory}/sample_submission.csv.zip  -d data/","metadata":{"id":"5lawOFYO_KUV","outputId":"282f1765-fc5a-4d88-e6f7-b68a843aba0a","execution":{"iopub.status.busy":"2022-07-13T22:20:38.289252Z","iopub.execute_input":"2022-07-13T22:20:38.291613Z","iopub.status.idle":"2022-07-13T22:20:43.599679Z","shell.execute_reply.started":"2022-07-13T22:20:38.291565Z","shell.execute_reply":"2022-07-13T22:20:43.598970Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf","metadata":{"id":"aGa2jmNmCjXf","execution":{"iopub.status.busy":"2022-07-13T22:20:43.603004Z","iopub.execute_input":"2022-07-13T22:20:43.603249Z","iopub.status.idle":"2022-07-13T22:20:49.036195Z","shell.execute_reply.started":"2022-07-13T22:20:43.603205Z","shell.execute_reply":"2022-07-13T22:20:49.035459Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## 1. Data Pipeline\n- Loading the datasets from CSVs\n- Preprocessing (Tokenization, Truncation & Padding)\n- Creating efficient data pipelines using tf.data","metadata":{}},{"cell_type":"code","source":"train_path = 'data/train.csv'\ntest_path = 'data/test.csv'\ntest_labels_path = 'data/test_labels.csv'\nsubm_path = 'data/sample_submission.csv'","metadata":{"id":"5FuPBMEX_Lxv","execution":{"iopub.status.busy":"2022-07-13T22:20:49.040153Z","iopub.execute_input":"2022-07-13T22:20:49.040392Z","iopub.status.idle":"2022-07-13T22:20:49.045481Z","shell.execute_reply.started":"2022-07-13T22:20:49.040346Z","shell.execute_reply":"2022-07-13T22:20:49.043213Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\ndf_train = pd.read_csv(train_path)\ndf_test = pd.read_csv(test_path)\ndf_test_labels = pd.read_csv(test_labels_path)\ndf_test_labels = df_test_labels.set_index('id')\n\ndf_train.head()","metadata":{"id":"50uZZZzH_NDV","outputId":"f925ad52-96d5-47a1-b573-db6fa12012ba","execution":{"iopub.status.busy":"2022-07-13T22:20:49.047109Z","iopub.execute_input":"2022-07-13T22:20:49.047554Z","iopub.status.idle":"2022-07-13T22:20:50.783599Z","shell.execute_reply.started":"2022-07-13T22:20:49.047361Z","shell.execute_reply":"2022-07-13T22:20:50.782877Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nbert_model_name = 'bert-base-uncased'\n\ntokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\nMAX_LEN = 128\n\ndef tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n    tokenized_sentences = []\n\n    for sentence in tqdm(sentences):\n        tokenized_sentence = tokenizer.encode(\n                            sentence,                  # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = max_seq_len,  # Truncate all sentences.\n                    )\n        \n        tokenized_sentences.append(tokenized_sentence)\n\n    return tokenized_sentences\n\ndef create_attention_masks(tokenized_and_padded_sentences):\n    attention_masks = []\n\n    for sentence in tokenized_and_padded_sentences:\n        att_mask = [int(token_id > 0) for token_id in sentence]\n        attention_masks.append(att_mask)\n\n    return np.asarray(attention_masks)\n\ninput_ids = tokenize_sentences(df_train['comment_text'], tokenizer, MAX_LEN)\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\nattention_masks = create_attention_masks(input_ids)","metadata":{"id":"g-8WnPfD_35q","outputId":"ef31e72c-9dc5-4e7d-875e-875758466538","execution":{"iopub.status.busy":"2022-07-13T22:20:50.785006Z","iopub.execute_input":"2022-07-13T22:20:50.785922Z","iopub.status.idle":"2022-07-13T22:28:14.113084Z","shell.execute_reply.started":"2022-07-13T22:20:50.785258Z","shell.execute_reply":"2022-07-13T22:28:14.112323Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nlabels =  df_train[label_cols].values\n\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=0, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=0, test_size=0.1)\n\ntrain_size = len(train_inputs)\nvalidation_size = len(validation_inputs)","metadata":{"id":"bMumeio9FFds","execution":{"iopub.status.busy":"2022-07-13T22:28:14.114415Z","iopub.execute_input":"2022-07-13T22:28:14.114710Z","iopub.status.idle":"2022-07-13T22:28:14.299137Z","shell.execute_reply.started":"2022-07-13T22:28:14.114652Z","shell.execute_reply":"2022-07-13T22:28:14.298371Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\nNR_EPOCHS = 1\n\ndef create_dataset(data_tuple, epochs=1, batch_size=32, buffer_size=10000, train=True):\n    dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n    if train:\n        dataset = dataset.shuffle(buffer_size=buffer_size)\n    dataset = dataset.repeat(epochs)\n    dataset = dataset.batch(batch_size)\n    if train:\n        dataset = dataset.prefetch(1)\n    \n    return dataset\n\ntrain_dataset = create_dataset((train_inputs, train_masks, train_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\nvalidation_dataset = create_dataset((validation_inputs, validation_masks, validation_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)","metadata":{"id":"H99t9YNDFcRq","execution":{"iopub.status.busy":"2022-07-13T22:28:14.300573Z","iopub.execute_input":"2022-07-13T22:28:14.301140Z","iopub.status.idle":"2022-07-13T22:28:21.388035Z","shell.execute_reply.started":"2022-07-13T22:28:14.300876Z","shell.execute_reply":"2022-07-13T22:28:21.387158Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## 2. BERT Model\n- Load the pretrained BERT base-model from Transformers library\n- Take the first hidden-state from BERT output (corresponding to CLS token) and feed it into a Dense layer with 6 neurons and sigmoid activation (Classifier). The outputs of this layer can be interpreted as probabilities for each of the 6 classes.","metadata":{}},{"cell_type":"code","source":"from transformers import TFBertModel\nfrom tensorflow.keras.layers import Dense, Flatten\n\nclass BertClassifier(tf.keras.Model):    \n    def __init__(self, bert: TFBertModel, num_classes: int):\n        super().__init__()\n        self.bert = bert\n        self.classifier = Dense(num_classes, activation='sigmoid')\n        \n    @tf.function\n    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        outputs = self.bert(input_ids,\n                               attention_mask=attention_mask,\n                               token_type_ids=token_type_ids,\n                               position_ids=position_ids,\n                               head_mask=head_mask)\n        cls_output = outputs[1]\n        cls_output = self.classifier(cls_output)\n                \n        return cls_output\n\nmodel = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(label_cols))","metadata":{"id":"3Zp6X7meF_T5","execution":{"iopub.status.busy":"2022-07-13T22:28:21.389376Z","iopub.execute_input":"2022-07-13T22:28:21.389653Z","iopub.status.idle":"2022-07-13T22:28:39.815649Z","shell.execute_reply.started":"2022-07-13T22:28:21.389610Z","shell.execute_reply":"2022-07-13T22:28:39.814615Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## 3. Training Loop\n- Use BinaryCrossentropy as loss function (is calculated for each of the output 6 output neurons ...that's like training 6 binary classification tasks at the same time) \n- Use the AdamW optimizer with 1-cycle-policy from the Transformers library\n- AUC evaluation metrics","metadata":{}},{"cell_type":"code","source":"import time\nfrom transformers import create_optimizer\n\nsteps_per_epoch = train_size // BATCH_SIZE\nvalidation_steps = validation_size // BATCH_SIZE\n\n# | Loss Function\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\nvalidation_loss = tf.keras.metrics.Mean(name='test_loss')\n\n# | Optimizer (with 1-cycle-policy)\nwarmup_steps = steps_per_epoch // 3\ntotal_steps = steps_per_epoch * NR_EPOCHS - warmup_steps\noptimizer = create_optimizer(init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)\n\n# | Metrics\ntrain_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\nvalidation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n\n@tf.function\ndef train_step(model, token_ids, masks, labels):\n    labels = tf.dtypes.cast(labels, tf.float32)\n\n    with tf.GradientTape() as tape:\n        predictions = model(token_ids, attention_mask=masks)\n        loss = loss_object(labels, predictions)\n\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables), 1.0)\n\n    train_loss(loss)\n\n    for i, auc in enumerate(train_auc_metrics):\n        auc.update_state(labels[:,i], predictions[:,i])\n        \n@tf.function\ndef validation_step(model, token_ids, masks, labels):\n    labels = tf.dtypes.cast(labels, tf.float32)\n\n    predictions = model(token_ids, attention_mask=masks, training=False)\n    v_loss = loss_object(labels, predictions)\n\n    validation_loss(v_loss)\n    for i, auc in enumerate(validation_auc_metrics):\n        auc.update_state(labels[:,i], predictions[:,i])\n                                              \ndef train(model, train_dataset, val_dataset, train_steps_per_epoch, val_steps_per_epoch, epochs):\n    for epoch in range(epochs):\n        print('=' * 50, f\"EPOCH {epoch}\", '=' * 50)\n\n        start = time.time()\n\n        for i, (token_ids, masks, labels) in enumerate(tqdm(train_dataset, total=train_steps_per_epoch)):\n            train_step(model, token_ids, masks, labels)\n            if i % 1000 == 0:\n                print(f'\\nTrain Step: {i}, Loss: {train_loss.result()}')\n                for i, label_name in enumerate(label_cols):\n                    print(f\"{label_name} roc_auc {train_auc_metrics[i].result()}\")\n                    train_auc_metrics[i].reset_states()\n        \n        for i, (token_ids, masks, labels) in enumerate(tqdm(val_dataset, total=val_steps_per_epoch)):\n            validation_step(model, token_ids, masks, labels)\n\n        print(f'\\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\n\n        for i, label_name in enumerate(label_cols):\n            print(f\"{label_name} roc_auc {validation_auc_metrics[i].result()}\")\n            validation_auc_metrics[i].reset_states()\n\n        print('\\n')\n\n        \ntrain(model, train_dataset, validation_dataset, train_steps_per_epoch=steps_per_epoch, val_steps_per_epoch=validation_steps, epochs=NR_EPOCHS)","metadata":{"id":"cJsVaX4qqoM0","outputId":"52dfd86b-a8f4-48f5-c481-a9db4f5c40fa","execution":{"iopub.status.busy":"2022-07-13T22:28:39.817564Z","iopub.execute_input":"2022-07-13T22:28:39.817911Z","iopub.status.idle":"2022-07-13T23:02:43.776885Z","shell.execute_reply.started":"2022-07-13T22:28:39.817849Z","shell.execute_reply":"2022-07-13T23:02:43.775894Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## 4. Run predictions on test-set & save submission","metadata":{}},{"cell_type":"code","source":"test_input_ids = tokenize_sentences(df_test['comment_text'], tokenizer, MAX_LEN)\ntest_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\ntest_attention_masks = create_attention_masks(test_input_ids)","metadata":{"id":"cdShHpPfH3es","outputId":"94b89833-244c-4cde-e6f6-637594ddef4b","execution":{"iopub.status.busy":"2022-07-13T23:02:43.778226Z","iopub.execute_input":"2022-07-13T23:02:43.778501Z","iopub.status.idle":"2022-07-13T23:09:22.724219Z","shell.execute_reply.started":"2022-07-13T23:02:43.778455Z","shell.execute_reply":"2022-07-13T23:09:22.722962Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"TEST_BATCH_SIZE = 32\ntest_steps = len(df_test) // TEST_BATCH_SIZE\n\ntest_dataset = create_dataset((test_input_ids, test_attention_masks), batch_size=TEST_BATCH_SIZE, train=False, epochs=1)\n\ndf_submission = pd.read_csv(subm_path, index_col='id')\n\nfor i, (token_ids, masks) in enumerate(tqdm(test_dataset, total=test_steps)):\n    sample_ids = df_test.iloc[i*TEST_BATCH_SIZE:(i+1)*TEST_BATCH_SIZE]['id']\n    predictions = model(token_ids, attention_mask=masks).numpy()\n\n    df_submission.loc[sample_ids, label_cols] = predictions","metadata":{"id":"RUAKdp2EKYlZ","outputId":"29f3b4bc-2c0c-43cb-f873-1b0db86865d3","execution":{"iopub.status.busy":"2022-07-13T23:09:22.725681Z","iopub.execute_input":"2022-07-13T23:09:22.725978Z","iopub.status.idle":"2022-07-13T23:21:25.708845Z","shell.execute_reply.started":"2022-07-13T23:09:22.725932Z","shell.execute_reply":"2022-07-13T23:21:25.708086Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_submission.to_csv('submission.csv')","metadata":{"id":"JgAv8do3PYYX","execution":{"iopub.status.busy":"2022-07-13T23:22:56.081828Z","iopub.execute_input":"2022-07-13T23:22:56.082296Z","iopub.status.idle":"2022-07-13T23:22:58.571671Z","shell.execute_reply.started":"2022-07-13T23:22:56.082115Z","shell.execute_reply":"2022-07-13T23:22:58.570915Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df_submission","metadata":{"execution":{"iopub.status.busy":"2022-07-13T23:26:48.606342Z","iopub.execute_input":"2022-07-13T23:26:48.606667Z","iopub.status.idle":"2022-07-13T23:26:48.631676Z","shell.execute_reply.started":"2022-07-13T23:26:48.606602Z","shell.execute_reply":"2022-07-13T23:26:48.630413Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}